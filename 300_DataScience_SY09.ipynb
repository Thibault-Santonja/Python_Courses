{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5333426b-d435-4092-9154-9938c35d7618",
   "metadata": {},
   "source": [
    "SY09\n",
    "===\n",
    "\n",
    "* **Statistiques**\n",
    "Rassembler et étudier des données afin d'en tirer des informations\n",
    "- Théorie du hasard\n",
    "- Théorie de la probabilité (très lié aux stats)\n",
    "- Descritpions de tableaux de données :\n",
    "  - Visuelle (histogramme, carte...)\n",
    "  - Valeur typique (dispersion, forme, positionnement)\n",
    "  - Ajustement\n",
    "  - Corrélation\n",
    "  - Indices\n",
    "\n",
    "\n",
    "* **Data Mining** / fouille de données\n",
    "Développement de l'info a rendu possible l'analyse en composantes principales (nécessite diagonalisation de matrice, longue sur des matrices grandes) et à données l'accès à de grandes quantitées de données. Les entrepot de données (Data Warehouse) ont accentué le besoin d'analyse. De plus, le développement de nouveau algos (Réseaux neuronaux, Suport Vector Machine...) a permis une avancée rapide.\n",
    "\n",
    "Le data mining consiste à analyser de grand jeux de données pour en extraire des informations (généralement pour aider la prise de décision), par exemple :\n",
    "- Risque de récidive d'une crise cardiaque\n",
    "- Risque de cancer de ...\n",
    "- Detection de fraude\n",
    "- Reconnaissance de code postal sur une enveloppe\n",
    "...\n",
    "\n",
    "\n",
    "Généralement le data mining est constitué de deux partie :\n",
    "- Une phase d'exploration des données (détection d'erreur, d'outliers (valeur atypique), selection de variables, codage de variable...) via des outils de représentation graphique, des outils de synthèse ou encore des outils de corrélation\n",
    "- Une phase d'apprentissage et de décision\n",
    "\n",
    "\n",
    "> Bonnes sources : Duda et al. (2001), Flury (1997), Govaert (2003), Govaert (2009), Hastie et al. (2001), Lebart et al. (1995) et Saporta (2006)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 1. Méthodes non-supervisées\n",
    "\n",
    "### 1.1. Principaux types de données\n",
    "Les données sont souvent représentés par des tableaux rectangulaires de nombres : individus - variables (en général)\n",
    "> une matrice X représentant un ensemble Omega de dimensions (n, p), n individus, p variables\n",
    "\n",
    "Les variables peuvent être :\n",
    "- Quantitative : des nombres dans R, où on distingue \n",
    "  - Les valeurs continues (une temperature)\n",
    "  - Les valeurs discrètes (une note à un examen)\n",
    "- Qualitative : des mots, qui peuvent être de deux types :\n",
    "  - qualitative nominale, comme un nom de ville \n",
    "  - qualitative ordinales, où il y a un ordre, par exemple un avis : bon > moyen > mauvais (on peut noter ici que l'on a 3 modalités (possibilités de valeurs))\n",
    "  > Il est important de distinguer les variables nominales et ordinales pour ne pas obtenir d'infos erronées ou manquer des informations\n",
    "\n",
    "* **Les variables binaires**\n",
    "Souvent codés 1 ou 0, on distingue une nouvelle fois deux cas de figure :\n",
    "- Les binaires symétriques (femme / homme)\n",
    "- Les binaires avec un ordre (présence / absence avec présence en général > absence)\n",
    "\n",
    "\n",
    "Pour étudier ces variables simultanément, il peut être intéressant de faire des pré-traitement (pour faciliter l'étude)\n",
    "- Quantitative -> Quantitative : \n",
    "  - *centrage-réduction* : rendre homogènes les variables en les **centrant** en 0 (soustraction par la moyenne), et en les réduisant (division de chaque valeurs par l'écart-type).\n",
    "  - Combiner des variables \n",
    "- Quantitative -> Qualitative : on peut découper un ensemble en plusieurs intervalles, définit selon :\n",
    "  - un **a priori** : age coupé de 0 à 18 puis 19 - 40, 41 - 65 et 65+\n",
    "  - selon la *forme* de l'histogramme (via les modes)\n",
    "  - découpage selon m intervalles de même longueur (d'effectif pas forcément égal)\n",
    "  - découpage en intervalles d'effectifs égaux (de longueur pas forcément égale)\n",
    "- Qualitative -> Binaire : plusieurs techniques existent :\n",
    "  - le codage disjonctif complet (+ réduction de tableau), le plus utilisé \n",
    "  - le codage additif, pour garder l'ordre des variables\n",
    "\n",
    "\n",
    "### 1.2. Tableaux de proximités\n",
    "Un tableau de proximité, est une matrice carrée représentant la ressemblance (ou la non ressemblance / disressemblance) entre plusieurs variable d'un ensemble Omega, ex : tableaux de distance routières, tableaux de corrélation...\n",
    "- Mesure de similarité : généralement, mesure de symétrie\n",
    "- Mesure de dissimilarité : la dissimilarité euclidienne ou distance euclidienne, plus deux point sont éloignés, moins ils sont similaires, distance ultramétrique, inégalité triangulaire\n",
    "\n",
    "Il existe plusieurs tableaux:\n",
    "- distance euclidienne\n",
    "- distance euclidienne pondérée\n",
    "- distance de Mahalanobis\n",
    "- distance « Manhattan » ou distance L1\n",
    "- distance de Chebychev ou distance L1\n",
    "- distance de Minkowski Lp\n",
    "\n",
    "Pour les qualitative, on peut utiliser \n",
    "- le X² (ou chi-deux)\n",
    "- la distance d = 1􀀀 proportion de modalités communes\n",
    "Et pour les binaires:\n",
    "- Csekanowski, Sorensen, Dice)\n",
    "- Hamman\n",
    "- Jaccard\n",
    "- Ochiai\n",
    "- Kulezynsky\n",
    "\n",
    "\n",
    "Enfin, on peut facilement passer de similarité à dissimilarité avec : dij = smax - sij \n",
    "\n",
    "\n",
    "### 1.3. Méthodes exploratoires élémentaires\n",
    "Voyons les principaux outils de statistique exploratoire (ou descriptive) ou EDA (Exploratory data analysis):\n",
    "\n",
    "#### 1.3.1. Quantitatif\n",
    "**Description monodimensionnelle**\n",
    "* Indicateurs numériques :\n",
    "  - Moyenne empirique\n",
    "  - Médiane\n",
    "  - Quantiles d'ordre p\n",
    "  - Écart-type empirique\n",
    "* Indicateurs visuels :\n",
    "  - Histogramme : la règle de Sturges recommande de prendre un nombre de classes égal à l’entier immédiatement supérieur ou égal à 1 + log2 n\n",
    "  - Estimation de la densité par la méthode des noyaux** \n",
    "  - Diagramme en boîte\n",
    "\n",
    "\n",
    "**Description bidimensionnelle** : étudier les liens entre deux variables\n",
    "- Covariance et corrélation empiriques\n",
    "- Histogramme bidimensionnel (nuage de points)\n",
    "\n",
    "\n",
    "**Description multidimensionnelle**\n",
    "- Matrice de covariance et de corrélation\n",
    "- Multiplot ou graphique matriciel (Seaborn)\n",
    "- Description 3-D (attention pb de projection (sous un angle on voit une similarité mais pas sous un autre angle))\n",
    "\n",
    "\n",
    "#### 1.3.2. Qualitatif\n",
    "**Description monodimensionnelle**\n",
    "- Camembert\n",
    "- Bâtons\n",
    "\n",
    "\n",
    "**Description bidimensionnelle** : \n",
    "- tableaux de contingence (compte le nombre d'apparition de chaque valeurs possibles (avec Pandas : `df.groupby(['column1']).sum()`))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.4. Représentation euclidienne des données\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 1.5. L’analyse en composantes principales\n",
    "\n",
    "On va chercher à réduire le nombre de variables et donc la complexité du problème. C'est par exemple ce que l'on fait quand on fait la moyenne scolaire (plutôt que de garder toutes les notes, on ne garde que la moyenne).\n",
    "\n",
    "Lorsque les variables sont toutes quantitatives, l’analyse en composantes principales (ACP) va chercher à résoudre ce problème en considérant que les nouvelles variables sont des combinaisons linéaires des variables initiales\n",
    "\n",
    "\n",
    "\n",
    "### 1.6. Positionnement multidimensionnel\n",
    "- Analyse factorielle d’un tableau de distances (l’AFTD)\n",
    "- Qualité de l’ajustement (Méthode du coude, Diagramme de Shepard)\n",
    "- Méthodes non linéaires (Fonctions Stress, Projection de Sammon)\n",
    "- Méthodes non métriques ou ordinales (Projection de Kruskal)\n",
    "- unfolding method, Individual differences\n",
    "\n",
    "\n",
    "### 1.7. La classification automatique\n",
    "La classification, à ne pas confondre avec le classement, est l’organisation d’un ensemble en classes homogènes ou classes naturelles. La classification automatique ou clustering recouvre l’ensemble des méthodes permettant la construction automatique de telles classifications. démarches sont généralement utilisées :\n",
    "- monothétique : On regroupe en classe les objets qui partagent certaines caractéristiques.\n",
    "- polythétique : On regroupe en classe les objets qui posséderont des caractéristiques « proches ».\n",
    "\n",
    "\n",
    "Les structures de classification peuvent être variées : partitions, suite de partitions emboîtées ou hiérarchie (ex : Dendrogramme), classes empiétantes ou recouvrement, classes de fortes densités, partitions floues (possibilité d'appartenance à plusieurs classes à différents degrès.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2. Méthodes supervisées\n",
    "- Reconnaissance de formes\n",
    "- Classification supervisée, discrimination\n",
    "- Régression\n",
    "\n",
    "\n",
    "Difficultés :\n",
    "- Sur apprentissage\n",
    "- Fléau de la dimension (plus il y a de dimensions, plus il faut de données)\n",
    "- Choix du modèle\n",
    "\n",
    "Classifieurs simples :\n",
    "- Euclidien\n",
    "- KNN\n",
    "\n",
    "\n",
    "\n",
    "### 2.1. Théorie bayésienne de la décision\n",
    "> Minimisation de la probabilité d’erreur\n",
    "\n",
    "\n",
    "### 2.2. Analyses discriminantes quadratique et linéaire\n",
    "- Estimateurs du maximum de vraisemblance (EMV)\n",
    "- Analyse discriminante linéaire\n",
    "- Analyse discriminante régularisée\n",
    "- Probabilité d’erreur de Bayes (Borne de Bhattacharyya)\n",
    "\n",
    "> sous l’hypothèse que les données suivent dans chaque classe une loi normale, et lorsque l’on suppose les matrices de variance identiques, la règle de Bayes peut être exprimée à l’aide de fonctions discriminantes linéaires. Cette méthode a l’avantage de fournir des estimations des probabilités a posteriori d’appartenance aux classes. Ces estimations sont d’autant plus précises que les hypothèses portant sur la distribution des données (normalité, forme ou égalité des matrices de covariance) sont vérifiées.\n",
    "\n",
    "\n",
    "\n",
    "### 2.3. Régression logistique\n",
    "> Plutôt que de faire des hypothèses sur les distributions conditionnelles fk, une autre approche\n",
    "consiste à estimer directement les probabilités d’appartenance aux classes\n",
    "\n",
    "Significativité des coefficients : \n",
    "- test de Wald\n",
    "- test du rapport de vraisemblance\n",
    "\n",
    "\n",
    "\n",
    "### 2.4. Arbres binaires\n",
    "> algorithme CART (Breiman et al. 1984)\n",
    "\n",
    "régularisation : simplifier l’arbre obtenu afin d’éviter le phénomène de sur-apprentissage\n",
    "\n",
    "\n",
    "Il est possible de construire plusieurs classifieurs à partir d’un même ensemble de données, en se basant sur différents modèles ou familles :\n",
    "- modèles gaussiens avec ou sans hypothèse d’homoscédasticité, avec ou sans hypothèse d’indépendance conditionnelle\n",
    "- ...\n",
    "\n",
    "Analyser le niveau de performance du classifieur : sélectionner le modèle minimisant l’espérance estimée du risque rn.\n",
    "\n",
    "\n",
    "niveau de performance du classifieur correspondant : Méthode de resubstitution, Méthode de l’ensemble de validation, Méthode de validation croisée (très courant), Méthode du bootstrap (très courant)\n",
    "\n",
    "\n",
    "### 2.5. La régression linéaire multiple\n",
    "Estimation des paramètres:  Moindre carrés \n",
    "\n",
    "Tests de significativité:\n",
    "- Loi des estimateurs sous hypothèse gaussienne\n",
    "- Test de significativité d’un coefficient de régression\n",
    "- Test de significativité du R2\n",
    "- Test d’une sous-hypothèse linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427b8b5-fa49-4d6b-966e-7fd5d40c9f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

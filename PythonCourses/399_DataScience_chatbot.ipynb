{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fa4af7c-437c-45c9-a7bb-600622a5baab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb503963-da4e-4144-a074-6e9c628394ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-ee95fe0b5088>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import random \n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c90d805-7418-4d62-bc03-baed7ac13d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"intents\": [\n",
    "             {\"tag\": \"greeting\",\n",
    "              \"patterns\": [\"Hello\", \"La forme?\", \"yo\", \"Salut\", \"ça roule?\"],\n",
    "              \"responses\": [\"Salut à toi!\", \"Hello\", \"Comment vas tu?\", \"Salutations!\", \"Enchanté\"],\n",
    "             },\n",
    "             {\"tag\": \"age\",\n",
    "              \"patterns\": [\"Quel âge as-tu?\", \"C'est quand ton anniversaire?\", \"Quand es-tu né?\"],\n",
    "              \"responses\": [\"J'ai 25 ans\", \"Je suis né en 1996\", \"Ma date d'anniversaire est le 3 juillet et je suis né en 1996\", \"03/07/1996\"]\n",
    "             },\n",
    "             {\"tag\": \"date\",\n",
    "              \"patterns\": [\"Que fais-tu ce week-end?\",\n",
    "\"Tu veux qu'on fasse un truc ensemble?\", \"Quels sont tes plans pour cette semaine\"],\n",
    "              \"responses\": [\"Je suis libre toute la semaine\", \"Je n'ai rien de prévu\", \"Je ne suis pas occupé\"]\n",
    "             },\n",
    "             {\"tag\": \"name\",\n",
    "              \"patterns\": [\"Quel est ton prénom?\", \"Comment tu t'appelles?\", \"Qui es-tu?\"],\n",
    "              \"responses\": [\"Mon prénom est Miki\", \"Je suis Miki\", \"Miki\"]\n",
    "             },\n",
    "             {\"tag\": \"goodbye\",\n",
    "              \"patterns\": [ \"bye\", \"Salut\", \"see ya\", \"adios\", \"cya\"],\n",
    "              \"responses\": [\"C'était sympa de te parler\", \"à plus tard\", \"On se reparle très vite!\"]\n",
    "             }\n",
    "]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c21826ea-503b-46eb-8218-d7c8ace5117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialisation de lemmatizer pour obtenir la racine des mots\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# création des listes\n",
    "words = []\n",
    "classes = []\n",
    "doc_X = []\n",
    "doc_y = []\n",
    "\n",
    "# parcourir avec une boucle For toutes les intentions\n",
    "# tokéniser chaque pattern et ajouter les tokens à la liste words, les patterns et\n",
    "# le tag associé à l'intention sont ajoutés aux listes correspondantes\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        tokens = nltk.word_tokenize(pattern)\n",
    "        words.extend(tokens)\n",
    "        doc_X.append(pattern)\n",
    "        doc_y.append(intent[\"tag\"])\n",
    "    \n",
    "    # ajouter le tag aux classes s'il n'est pas déjà là \n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "\n",
    "# lemmatiser tous les mots du vocabulaire et les convertir en minuscule\n",
    "# si les mots n'apparaissent pas dans la ponctuation\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "\n",
    "# trier le vocabulaire et les classes par ordre alphabétique et prendre le\n",
    "# set pour s'assurer qu'il n'y a pas de doublons\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aee991ea-8ae1-4207-ac35-a588561f4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liste pour les données d'entraînement\n",
    "training = []\n",
    "out_empty = [0] * len(classes)\n",
    "\n",
    "# création du modèle d'ensemble de mots\n",
    "for idx, doc in enumerate(doc_X):\n",
    "    bow = []\n",
    "    text = lemmatizer.lemmatize(doc.lower())\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "\n",
    "    # marque l'index de la classe à laquelle le pattern atguel est associé à\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes.index(doc_y[idx])] = 1\n",
    "\n",
    "    # ajoute le one hot encoded BoW et les classes associées à la liste training\n",
    "    training.append([bow, output_row])\n",
    "\n",
    "# mélanger les données et les convertir en array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "# séparer les features et les labels target\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13db04a2-519f-4d90-9101-3b3469112f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition de quelques paramètres\n",
    "input_shape = (len(train_X[0]),)\n",
    "output_shape = len(train_y[0])\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "377dae90-a4bd-4b04-ab00-67696a44b1af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ab1c139fb4a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# modèle Deep Learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# modèle Deep Learning\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(output_shape, activation = \"softmax\"))\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4404f158-5350-4914-9ccd-5deb9da9e8af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-3caefa261807>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc3e1887-4f10-4d62-a9a2-ab2e4fadaef0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-905af35b1111>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# entraînement du modèle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# entraînement du modèle\n",
    "model.fit(x=train_X, y=train_y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94dafecb-3ba2-482d-ae52-65c3d5e05f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def bag_of_words(text, vocab): \n",
    "    tokens = clean_text(text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for w in tokens: \n",
    "        for idx, word in enumerate(vocab):\n",
    "            if word == w: \n",
    "                bow[idx] = 1\n",
    "    return np.array(bow)\n",
    "\n",
    "def pred_class(text, vocab, labels): \n",
    "    bow = bag_of_words(text, vocab)\n",
    "    result = model.predict(np.array([bow]))[0]\n",
    "    thresh = 0.2\n",
    "    y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
    "\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]])\n",
    "    return return_list\n",
    "\n",
    "def get_response(intents_list, intents_json): \n",
    "    tag = intents_list[0]\n",
    "    list_of_intents = intents_json[\"intents\"]\n",
    "    for i in list_of_intents: \n",
    "        if i[\"tag\"] == tag:\n",
    "            result = random.choice(i[\"responses\"])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbec99ef-abd7-4ab7-820e-8f12ed7b6cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lancement du chatbot\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    intents = pred_class(message, words, classes)\n",
    "    result = get_response(intents, data)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51d104e-03b0-4164-be8b-ac00d09583f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

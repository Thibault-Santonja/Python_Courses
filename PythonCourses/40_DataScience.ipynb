{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataScience avec Python\n",
    "===\n",
    "\n",
    "Bonjour et bienvenu, avant de commencer ce cours, n'hésitez pas à jeter un oeil au cours sur Scikit ainsi que sur Numpy, Pandas et sur les plots, sachant que ces librairies seront très utilisées."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Démarche basique\n",
    "\n",
    "Une démarche de base à suivre est la suivante :\n",
    "1. Définir un objectif **mesurable**:\n",
    "    > Par exemple, prédire *y* avec 90% d'exactitude, une précision (prévient les faux positifs) de 60% et un recall (préviens les faux négatif) de 80% \n",
    "2. Exploration des données : EDA (Exploratory Data Analysis) comprendre au maximum les données et le dataset\n",
    "    - Analyse de la forme\n",
    "        - Identifier la target\n",
    "        - Nombre de lignes / colonnes\n",
    "        - Nombres de variables discrètes / continues\n",
    "        - Valeurs manquantes\n",
    "    - Analyse de fond\n",
    "        - Visualisation de la target (histogramme / boxplot)\n",
    "        - Compréhension des variables (On peut regarder sur internet)\n",
    "        - Visualisation des relations features-target (histogramme / boxplot)\n",
    "        - Identification des outliers\n",
    "3. Pre traitement des données : transformer le dataset pour qu'il soit plus propice pour le modèle\n",
    "    - Train set / Test set\n",
    "    - Élimination des NaN\n",
    "    - Encodage\n",
    "    - Suppression des outliers (pas forcément à la première itération)\n",
    "    - Feature Selection (supprimer les variables redondantes ou a variance faible)\n",
    "    - Feature Engineering (combiner des variables / split des variable / polynomial feature + PCA)\n",
    "    - Feature Scaling (Normaliser les données)\n",
    "4. Modelling : Choisir un modèle, l'entrainer, l'améliorer\n",
    "    - Définir une fonction d'évaluation\n",
    "    - Entrainement de différents modèles\n",
    "    - Optimisation avec GridSearchCV\n",
    "    - Analyse des erreurs et retour au preprocessing / EDA si nécessaire\n",
    "    - Learning curve (toujours à vérifier pour savoir si on manque ou non de données) et prise de décision\n",
    "\n",
    "Il existe de nombreuses méthodes, plus formalisées, pour gérer un projet de Data Science. Le plus connu est **CRISP-DM** (CRoss Industries Standard Process for Data Mining). Mais de manière générale, on a toujours les étapes ci-dessus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Démarche détaillée\n",
    "\n",
    "La Data Science c'est donc avant tout de lcountience des données. Ainsi il est important d'adopter une bonne démarche pour ne pas être perdu et tourner en rond. De plus, jouer avec les données et leur donner une signification (les valoriser) peut etre très dangereux. Il faut vraiment faire attention à ce que l'on fait dire aux données.\n",
    "\n",
    "De manière générale, voici une démarche à suivre plus détaillée qu'en introduction :\n",
    "1. **Définir le problème** : Il est important de se questionner sur le le problème et le projet avant de commencer, pour d'une part, bien le comprendre, et d'autre part, pour bien l'analyser. On peut utiliser la fameuse technique de gestion de projet des *5 pourquoi* afin de pousser l'analyse et répondre aux questions suivantes :\n",
    "    - Quel est le problème ?\n",
    "    - Quelle serait la valeur ajoutée ?\n",
    "    - Quels sont les risques / dangers, notamment éthiques du projet ?\n",
    "    - Avec et pour qui est fait le projet ? Savoir qui est le destinataire peut aider à comprendre les intentions non dites\n",
    "    - Quelles sont les ressources à disposition ? Humaines, machines...\n",
    "    - Quel est le type de problème ?\n",
    "    > Enfin, on peut définir un ou des objectifs mesurable ainsi que commencer à planifier le projet\n",
    "2. **Obtenir et explorer les données** : On peut créer et utiliser ici un ETL (Extract, Transform, Load) afin d'automatiser la récupération, le nettoyage et la première analyse de la donnée. Les principales étapes ici sont :\n",
    "    - Identifier les données dont on a besoin et les récupérer :\n",
    "        - Sont-elles disponibles en interne ? On a besoin de l'accès\n",
    "        - Sont-elles disponibles librement ? On les collecte\n",
    "        - Sont-elles disponibles à l'achat ? On achète ou pas ?\n",
    "    - Explorer, exploiter et analyser les données (EDA (Exploratory Data Analysis)) :\n",
    "        - Documenter les dataset et leur qualité (noms de colonnes, nombre de données, utilisabilité de la donnée...)\n",
    "        - Nettoyer le dataset (et documenter ce qui a été nettoyer et la stratégie de nettoyage)\n",
    "        - Faire du pre-traitement pour optimiser le modelling\n",
    "        - Combiner les datasets dans des vues et pourquoi pas, load ces dernières dans un cloud\n",
    "        - Visualiser les données (on peut faire cela plusieurs fois, notamment pour documenter le dataset et sa qualité) et première analyse des clients (y a t'il des relations qui ressortent, des correlations, des clusters...)\n",
    "    > Ici, des notebooks Python et l'utilisation de Seaborn prend tout son intérêt. Le but n'est pas de faire une analyse précise mais de commencer rapidement à étudier les données. En effet, le plus important de cette partie est de correctement nettoyer et documenter les données.\n",
    "3. **Modelling et Minimal Viable Value** : Plutôt que de faire un gros modèle tout de suite qui prend du temps à entrainer et qui pourra ne pas être efficace ou ne pas répondre au besoin, préférer y aller par étapes. C'est finalement le but des méthodes agiles et notamment du SCRUM. \n",
    "    - Chercher la **Minimal Viable Value** : comment avec très peu d'efforts, on peut créer une grosse plus-value ? Ceci permet de rapidement tester si l'on est dans la bonne voie, de se préparer pour la suite, d'avoir quelque chose à montrer au client et de valider, avec ce dernier, que l'on a bien compris son besoin et que l'on est dans la bonne voie:\n",
    "        - Le modèle est-il performant\n",
    "        - Le modèle est-il intéressant, \"parlant\"\n",
    "    - Tester le modèle (et surtout le Minimal Viable Value pour savoir si on peut continuer) :\n",
    "        - *Lab* validation : Tester le modèle en environnement controllé (Train set / Validation set)\n",
    "        - *Wild* validation : Vérifier qu'il fonctionne toujours dans le monde réel\n",
    "    > Une fois que l'on a déjà une base validée, on peut continuer et être plus confiant pour grossir le modèle, toujours de manière incrémentale (plus de données, plus de prédictions...)\n",
    "4. **Déployer et améliorer le modèle** : Donc livrer le produit et faire de l'amélioration continue.\n",
    "    - Le déploiement peut être très varié, un simple tableau / dashboard, jusqu'au Cloud Scalable pour des millions de personnes avec une API.\n",
    "    - L'amélioration peut consister à étendre le modèle à de nouveaux use cases, ajouter de nouvelles features, tester de nouvelle techniques de modelling...\n",
    "    - **Data Science Ops** afin de capitaliser sur ce qui a été fait. \n",
    "        - Documenter ce qui a été fait avec du recul (difficultés, facilités...) et assurer la compréhension du modèle\n",
    "        - DevOps sur le software (màj, patches, sécurité, tests...)\n",
    "        - Monitoring de la performance du modèle et des données (le monde réel change en continue, les données évoluent donc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thibault **Santonja**\n",
    "\n",
    "2021"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
